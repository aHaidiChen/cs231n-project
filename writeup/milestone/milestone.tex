\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{4321}
\begin{document}


%%%%%%%%% TITLE
\title{draw2pix: Generative Networks for Bad Artists}

\author{Cedrick Argueta\\
Stanford University\\
450 Serra Mall, Stanford, CA 94305\\
{\tt\small cedrick@cs.stanford.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Kevin Wang\\
Stanford University\\
450 Serra Mall, Stanford, CA 94305\\
{\tt\small kwang98@stanford.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% BODY TEXT
\section{Introduction}
We are investigating unpaired image-to-image translation with generative adversarial networks.
This work was demonstrated in \cite{cycleGAN}.
Work done in the same lab \cite{pix2pix} demonstrates how we can use paired images in training to create an image-to-image mapping from one domain to another.
The specific image-to-image task that we're aiming to implement is \href{https://affinelayer.com/pixsrv/}{this}, which demonstrates a couple examples of sketch-to-photo translations trained using the pix2pix model.
We aim to do something similar, using CycleGAN to instead train on unpaired images.
The relaxation that unpaired training gives us allows for easier creation of datasets and combinations of domains -- we can simply swap a whole domain and retrain rather than find image-to-image pairs for that specific translation task.
Our project differs from \cite{bicyclegan} in that we aren't doing multi-modal image translation, just from one domain to another. 
We are essentially implementing the demo from \cite{pix2pix} with an unpaired dataset and architecture matching that of \cite{cycleGAN}

\section{Related Work}
% TODO: gan, cogan citation
describe some of these, how they relate \\
The original Generative Adversarial Network (GAN) proposed by Goodfellow et al. \cite{gan}  proposes a network that learns an approximation of a distribution of data $p_{X}$.
It does so by training two networks - a generator network and a discriminator network.
These networks are trained adversarially, i.e. the discriminator's objective is to discern between real images and the generator's images, and the generator's objective is to generate images that fool the discriminator.

Generative networks are not limited to simple distributions.
Work done by Liu et al. \cite{cogan} demonstrated the ability of generative networks to learn a joint distribution between variables, through the use of multiple generator and discriminator networks.
This can be used to generate pairs $x_1 \sim p_{X_1}, x_2 \sim p_{X_2}$ of images that belong to the joint distribution just by using the marginal distributions.

Zhu et al. \cite{cycleGAN} goes a step further by enforcing cycle consistency between pairs created, i.e. by ensuring that generated images can be run through another generator to create the paired image.
CycleGAN effectively learns a mapping function $G : X \rightarrow Y$ for images in domains $X$ and $Y$.

The networks discussed earlier are performing unsupervised iamge-to-image translation in that no labels are given to pair images from domain $X$ and domain $Y$ together.
When these labels are given, supervised methods like those described by Isola et al. \cite{pix2pix} are possible through the use of conditional GANs.

\section{Problem Statement}
describe image-to-image translation
why do unpaired: easier for dataset collection, labels are expensive
                 because of this, allows for more general mappings


\section{Technical Approach}
We aim to use CycleGAN \cite{cycleGAN} for unpaired image-to-image translation.
More precisely, we will train networks to learn functions $G : X \rightarrow Y$ and $F: Y \rightarrow X$.
This will allow us to take an image $x_1 \in X$ and generate its pair $y_1 \in Y$, and vice versa.
We let the true distributions of the domains be $p_X$ and $p_Y$.
Just as in \cite{gan}, we will use discriminators $D_X$ and $D_Y$ as adversarial networks to the generators, which we will train to distinguish between elements in $X$ and $p_X$ and $Y$ and $p_Y$, respectively.
CycleGAN uses two losses per generator function.
The first loss is adversarial, where the loss function for $G$ and its discriminator $D_Y$ is:
\begin{multline}
\displaystyle \mathcal{L}_{GAN}(G, D_Y, X, Y) = \\
\mathbb{E}_{y \sim p_Y}[\log{D_Y(y)}]
+ \mathbb{E}_{x \sim p_X}[\log{1 - D_Y(G(x))}]
\end{multline}.
and the objective is $\displaystyle \min_{G} \max_{D_Y} \mathcal{L}_{GAN}$.
We can apply this without loss of generality to $F$ and its discriminator $D_X$.
Because it is possible for a network to map an input image $x$ to multiple images in $Y$, we use a cycle consistency loss to guarantee that there is a unique mapping.
Specifically, we wish to enforce that for images $x$ and $y$ $F(G(x)) \approx x$ (forward cycle consistency) and $G(F(y)) \approx y$ (backward cycle consistency).
The loss used is:
\begin{multline}
\displaystyle \mathcal{L}_{cyc} = \\
\mathbb{E}_{x \sim p_X}[\Vert{F(G(x)) - x}\Vert_1]
+ \mathbb{E}_{y \sim p_Y}[\Vert{G(F(y)) - y}\Vert_1]
\end{multline}.

Then the full loss function is:
\begin{multline}
\displaystyle \mathcal{L}(G, F, D_X, D_Y) = \\
\mathcal{L}_{GAN}(G, D_Y, X, Y)
+ \mathcal{L}_{GAN}(F, D_X, Y, X)
+ \lambda\mathcal{L}_{cyc}(G, F)
\end{multline},
where $\lambda$ is a parameter that determines the importance of cyclic consistency loss vs. adversarial loss.
Minimizing this loss through
\begin{equation}
\displaystyle G^*, F^* = \arg\min_{G, F}\max_{D_X, D_Y} \mathcal{L}(G, F, D_X, D_Y)
\end{equation}
yields $G^*$ and $F^*$, our optimal mapping functions.
\section{Dataset}
describe sketchy dataset

\section{Baseline Results}
describe cogan method, results

{\small
\bibliographystyle{ieee}
\bibliography{ref}
}

\end{document}
