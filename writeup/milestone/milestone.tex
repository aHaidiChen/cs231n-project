\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{4321}
\begin{document}


%%%%%%%%% TITLE
\title{draw2pix: Generative Networks for Bad Artists}

\author{Cedrick Argueta\\
Stanford University\\
450 Serra Mall, Stanford, CA 94305\\
{\tt\small cedrick@cs.stanford.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Kevin Wang\\
Stanford University\\
450 Serra Mall, Stanford, CA 94305\\
{\tt\small kwang98@stanford.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% BODY TEXT
\section{Introduction}
We are investigating unpaired image-to-image translation with generative adversarial networks.
This work was demonstrated in \cite{cycleGAN}.
Work done in the same lab \cite{pix2pix} demonstrates how we can use paired images in training to create an image-to-image mapping from one domain to another.
The specific image-to-image task that we're aiming to implement is \href{https://affinelayer.com/pixsrv/}{this}, which demonstrates a couple examples of sketch-to-photo translations trained using the pix2pix model.
We aim to do something similar, using CycleGAN to instead train on unpaired images.
The relaxation that unpaired training gives us allows for easier creation of datasets and combinations of domains -- we can simply swap a whole domain and retrain rather than find image-to-image pairs for that specific translation task.
Our project differs from \cite{bicyclegan} in that we aren't doing multi-modal image translation, just from one domain to another. 
We are essentially implementing the demo from \cite{pix2pix} with an unpaired dataset and architecture matching that of \cite{cycleGAN}

\section{Related Work}
describe some of these, how they relate \\
\href{https://arxiv.org/abs/1406.2661}{GAN} \\
The original Generative Adversarial Network (GAN) proposed by Goodfellow et al. proposes a network that learns an approximation of a distribution of data $p_{X}$.
\href{https://arxiv.org/pdf/1703.10593}{CycleGAN} \\
\href{https://arxiv.org/abs/1611.07004}{pix2pix} \\
\href{https://affinelayer.com/pixsrv/}{pix2pix demo} \\
\href{https://arxiv.org/abs/1801.02753}{SketchyGAN} \\
\href{https://arxiv.org/abs/1606.07536}{CoGAN} \\

\section{Problem Statement}
describe image-to-image translation, use cases, motivation

\section{Technical Approach}
describe cyclegan

\section{Dataset}
We will be using a subset of the Sketchy dataset, as described in \cite{sangkloy2016sketchy}. The dataset consists of a crowd-sourced collection of sketch-photo pairs. The total collection consists of 75,471 sketches of 12,500 photos (125 object categories). We will be working specifically with the 'tree' subset, which consists of 100 photos and 534 sketches. Although there is a naturally defined pairing between photos and sketches, we will be ignoring this mapping in our implementation of the unpaired translation.

\section{Baseline Results}
describe cogan method, results

{\small
\bibliographystyle{ieee}
\bibliography{ref}
}

\end{document}
