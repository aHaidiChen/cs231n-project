\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Project Proposal Notes}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Cedrick Argueta \\
  Department of Computer Science\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{cedrick@cs.stanford.edu} \\
  \And
  Kevin Wang \\
  Department of Physics\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{kwang98@stanford.edu} \\
}
\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}

 
\end{abstract}


\section{What is the problem that you will be investigating? Why is it interesting?}

We are investigating unpaired image-to-image translation with generative adversarial networks.
This work was demonstrated in \cite{cycleGAN} and \cite{pix2pix}.
The pix2pix paper demonstrates how we can use paired images in training to create an image-to-image mapping from one domain to another.
With CycleGAN, we can shift to using unpaired images for each domain.

\section{What reading will you examine to provide context and background?}

\href{https://arxiv.org/pdf/1703.10593.pdf}{CycleGAN} \\
\href{https://arxiv.org/abs/1611.07004}{pix2pix} \\
\href{https://arxiv.org/abs/1801.02753}{SketchyGAN} \\
\href{https://arxiv.org/abs/1903.07291}{SPADE} \\
\href{https://quickdraw.withgoogle.com/}{Google Drawing Guessing Game} \\

\section{What method or algorithm are you proposing? If there are existing implementations, will you use them and how? How do you plan to improve or modify such implementations? You don't have to have an exact answer at this point, but you should have a general sense of how you will approach the problem you are working on. }

We will use CycleGAN to generate the image-to-image translations.
We use the sketch as input, and then the GAN produces an image that it believes is in the target domain. 
The discriminator judges this output, deciding whether the generated image was real or fake.
When a threshold has been reached, we have a GAN capable of producing fake images that belong to the target domain that corresponds to the input image, because of the cyclic property of CycleGANs.

\href{https://github.com/yunjey/mnist-svhn-transfer}{Simple CycleGAN} \\

\section{What data will you use? If you are collecting new data, how will you do it?}

We will combine labeled data from TU-Berlin sketch dataset \cite{tu-berlin-sketch} and ImageNet \cite{imagenet} to create our domains. 

\section{How will you evaluate your results? Qualitatively, what kind of results do you expect (e.g. plots or figures)? Quantitatively, what kind of analysis will you use to evaluate and/or compare your results (e.g. what performance metrics or statistical tests)? }

Qualitative: whether the picture we draw can be translated to a realistic image. \\
Quantitative: Inception Score, Fr\'echet Inception Distance

\bibliography{ref}
\bibliographystyle{plain}


\end{document}

